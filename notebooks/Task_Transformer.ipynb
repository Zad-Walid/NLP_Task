{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dW2Gdbp5Lhew"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "5mZ_vTwwZKRu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yJeT3UjPcBb"
      },
      "source": [
        "# Name Entity Recognition (ner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Q1mgZysoLpMU"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Microsoft Corporation announced a $10 billion investment in OpenAI on Monday.\n",
        "CEO Satya Nadella stated that this partnership will accelerate AI innovation worldwide.\n",
        "The announcement was made at the companyâ€™s headquarters in Redmond, Washington.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A__hjF94L42W",
        "outputId": "7e96357b-61e6-4b93-f89f-0ed491626191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HsknL-ZL64P",
        "outputId": "d9f043cb-87a8-4b39-a6a9-784f30406a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer , grouped_entities = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZXsw0sJL9w1",
        "outputId": "4cbc29ae-b936-47d9-9df0-1459cfbceaaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity_group': 'ORG', 'score': np.float32(0.9992714), 'word': 'Microsoft Corporation', 'start': 0, 'end': 21}, {'entity_group': 'ORG', 'score': np.float32(0.90710396), 'word': 'OpenAI', 'start': 60, 'end': 66}, {'entity_group': 'PER', 'score': np.float32(0.9620562), 'word': 'Satya Nadella', 'start': 82, 'end': 95}, {'entity_group': 'MISC', 'score': np.float32(0.99135613), 'word': 'AI', 'start': 141, 'end': 143}, {'entity_group': 'LOC', 'score': np.float32(0.95440245), 'word': 'Redmond', 'start': 225, 'end': 232}, {'entity_group': 'LOC', 'score': np.float32(0.99818677), 'word': 'Washington', 'start': 234, 'end': 244}]\n"
          ]
        }
      ],
      "source": [
        "ner_results = nlp(text)\n",
        "print(ner_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVowIbpXMP1v",
        "outputId": "f4358cd6-88f7-492e-fd3f-33cf3907dc26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Microsoft Corporation (ORG) - Score: 1.00\n",
            "OpenAI (ORG) - Score: 0.91\n",
            "Satya Nadella (PER) - Score: 0.96\n",
            "AI (MISC) - Score: 0.99\n",
            "Redmond (LOC) - Score: 0.95\n",
            "Washington (LOC) - Score: 1.00\n"
          ]
        }
      ],
      "source": [
        "for entity in ner_results:\n",
        "    print(f\"{entity['word']} ({entity['entity_group']}) - Score: {entity['score']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "VOEVfc3vNk01"
      },
      "outputs": [],
      "source": [
        "text_2 = \"\"\"Dr. Sarah Lee from Johns Hopkins Hospital presented a study on the use of Remdesivir for COVID-19 patients.\n",
        "The findings were published in The Lancet on March 3, 2023.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSOh14DyO4Y_",
        "outputId": "a2fbe261-43b2-4ea0-fe2d-c0ed10091b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity_group': 'PER', 'score': np.float32(0.99948883), 'word': 'Sarah Lee', 'start': 4, 'end': 13}, {'entity_group': 'LOC', 'score': np.float32(0.9991717), 'word': 'Johns Hopkins Hospital', 'start': 19, 'end': 41}, {'entity_group': 'MISC', 'score': np.float32(0.6587718), 'word': 'Re', 'start': 74, 'end': 76}, {'entity_group': 'ORG', 'score': np.float32(0.50892603), 'word': '##desiv', 'start': 77, 'end': 82}, {'entity_group': 'MISC', 'score': np.float32(0.6624677), 'word': 'CO', 'start': 89, 'end': 91}, {'entity_group': 'ORG', 'score': np.float32(0.99575424), 'word': 'The Lancet', 'start': 139, 'end': 149}]\n"
          ]
        }
      ],
      "source": [
        "ner_results_2 = nlp(text_2)\n",
        "print(ner_results_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ2tWTYXO8EM",
        "outputId": "4ef5d2b5-41f6-4ff8-eb38-79e31baab9f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sarah Lee (PER) - Score: 1.00\n",
            "Johns Hopkins Hospital (LOC) - Score: 1.00\n",
            "Re (MISC) - Score: 0.66\n",
            "##desiv (ORG) - Score: 0.51\n",
            "CO (MISC) - Score: 0.66\n",
            "The Lancet (ORG) - Score: 1.00\n"
          ]
        }
      ],
      "source": [
        "for entity in ner_results_2:\n",
        "    print(f\"{entity['word']} ({entity['entity_group']}) - Score: {entity['score']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C_P03jyPgv-"
      },
      "source": [
        "# Text Summerization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "blPRtLj-RZm6"
      },
      "outputs": [],
      "source": [
        "Article = \"\"\"BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n",
        "BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).\n",
        "This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvqdkctBPgbF",
        "outputId": "59e6b855-40f4-409e-982f-d7234485f200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "model = \"facebook/bart-large-cnn\"\n",
        "summarizer = pipeline(\"summarization\", model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJDqj5o2RPLD",
        "outputId": "a88b3996-a47e-4589-f84c-8af5ab4b6b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': 'BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder. BART is pre-trained by corrupting text with an arbitrary noising function, and learning a model to reconstruct the original text.'}]\n"
          ]
        }
      ],
      "source": [
        "print(summarizer(Article, max_length=130, min_length=30, do_sample=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jLSMvMGRzkQ",
        "outputId": "d9d6e8ee-000f-44f8-83d0-129c8d7c7a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder. BART is pre-trained by corrupting text with an arbitrary noising function, and learning a model to reconstruct the original text.\n"
          ]
        }
      ],
      "source": [
        "summary = summarizer(Article, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44pqyhelSVgr",
        "outputId": "aa4834c2-5268-4079-c492-dea319fa6750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "model_2 = \"google/pegasus-xsum\"\n",
        "summarizer_2 = pipeline(\"summarization\", model=model_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHtAOPqMSyx9",
        "outputId": "69dc04ff-3071-4e18-b266-e71406cb03d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': 'The worldâ€™s largest software company has teamed up with a leading artificial intelligence (AI) firm, OpenAI, to create the worldâ€™s most'}]\n"
          ]
        }
      ],
      "source": [
        "print(summarizer_2(text, max_length=32, min_length=30, do_sample=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBFmOvW6S7Yi",
        "outputId": "9f4e7283-52c9-4ba0-e473-70e571a9b60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The worldâ€™s largest software company has teamed up with a leading artificial intelligence (AI) firm, OpenAI, to create the worldâ€™s most\n"
          ]
        }
      ],
      "source": [
        "summary_2 = summarizer_2(text, max_length=32, min_length=30, do_sample=False)[0]['summary_text']\n",
        "print(summary_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CXrvdOjUEfQ"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RybtL9ogUIwl",
        "outputId": "6ee52c97-04e5-4424-86b8-e9e4bc2bc61a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbcF2ELxVJss",
        "outputId": "05020b09-50ad-4db7-d6a4-e34bed930f73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9984573125839233}]\n"
          ]
        }
      ],
      "source": [
        "print(sentiment_analysis(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGdk4mjjVTqn",
        "outputId": "33abe8be-42fa-4fd6-916a-b28187987f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9985944628715515}]\n"
          ]
        }
      ],
      "source": [
        "print(sentiment_analysis(summary_2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}